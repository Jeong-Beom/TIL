## 계층적 군집분석(Hiarchical Clustering)

ㆍ**군집분석** : 비지도학습의 대표적인 예로써, y label이 없는 상태에서 x 변수(특성변수)를 비슷한 집단끼리 묶어서 분할하는 방법

- 어떤 개체나 대상들을 유사성(similarity) 또는 비유사성(dissimilarity)에 의하여 유사한 특성을 지닌 개체들을 집단화하는 방법
- 각 군집의 특성, 군집간의 차이 등에 대한 탐색대상으로, 집단에 대한 심화된 이해가 목적
- 특이 군집의 발견, 결측값의 보정 등에도 사용될 수 있음

ㆍ군집의 조건 : 동일 군집에 속한 개체끼리는 유사한 속성이 많음 / 다른 군집에 속하는 개체끼리는 유사한 속성이 적음

ㆍ**계층적 군집분석의 개요**

- 병합적(agglomerative) VS 분할적(divisive)
  - 병합적 : 개체 간 거리가 가까운 개체끼리 차례로 묶어주는 방법으로 군집을 정의
  - 분할적 : 개체 간 거리가 먼 개체끼리 나누어 가는 방법으로 군집을 정의
  - 계층적 군집분석에서는 병합적 방법이 주로 사용됨
- 개체 간 거리 및 군집간의 거리의 정의
  - 개체 간 거리 
    - 유클리디안 거리
    - 맨해튼 거리
    - 민코우스키 거리
  - 군집 간 거리
    - 단일연결법(최단 연결법, single linkage)
      1. 두 군집 C1과 C2의 거리는 dc1c2 = min{d(x, y) | x ∈ C1, y ∈ C2}로 정의
    - 완전 연결법(최장 연결법, complete linkage)
      1. 두 군집 C1과 C2의 거리는 dc1c2 = max{d(x, y) | x ∈ C1, y ∈ C2}로 정의
    - 평균 연결법(average linkage)
      1. 두 군집 C1과 C2의 거리는 두 군집의 모든 개체간 거리들의 평균으로 정의
    - 중심 연결법(centroid linkage)
      1. 두 군집 C1과 C2의 거리는 두 군집의 중심 사이의 거리로 정의
    - ward 연결방법(ward linkage)
      1. SSEk를 군집 k의 중심으로부터 해당 군집 각 개체간의 거리 제곱합으로 정의한 뒤, SSE = ∑(k=1 ~ K)SSEk로 정의
      2. K개 중 2개의 군집을 하나의 군집으로 묶었을 때 오차제곱합이 증가하는 정도를 두 군집간의 거리로 정의



## 비계층적 군집분석(K-means Clustering)

ㆍ분석시간이 오래걸리는 계층적 군집분석과 달리 빠르게 결과를 도출해낼 수 있음

ㆍ**K-평균 군집분석**

- 사전에 결정된 군집 수 k에 기초하여, 전체 데이터를 상대적으로 유사한 k개의 군집으로 구분

- 계층적 방식에 비하여 계산량이 적고, 대용량 데이터를 빠르게 처리할 수 있어 실무에 적합함

- 사전에 적절한 군집 수 k에 대한 예상이 필요

- 초기에 군집 중심이 어디로 지정되는지에 따라 최종결과가 영향을 많이 받음

- 잡음이나 이상치의 여향을 많이 받음

- **적용절차**

  - 개체를 k개의 초기 군집으로 나눈다.
  - 각 군집의 중심(centroid)을 계산한 뒤 모든 개체들을 각 군집의 중심에 가장 가까운 군집에 할당시킨다.
  - 새로운 개체를 받아들이거나 잃은 군집의 중심을 다시 계산한다.
  - 위 과정을 더이상의 재배치가 생기지 않을 때까지 반복한다.

- **적절한 군집 수의 결정**

  - 오차 제곱합(SSE, sum of squared error)

    - 각 군집 내 개체들과 해당 군집 중심점과의 거리를제곱한 값들의 합. 

    - 오차제곱합이 작을 수록 군집내 유사성이 높아 잘 응집된 것임

    - 군집수 k에 따른 SSE의 변화를 Elbow 차트로 시각화한 뒤, SSE가 급격히 감소하다가 완만해지기 

      시작하는 시점의 k를 적정 군집수로 판단함



## 단순선형회귀분석(Simple Linear Regression)

ㆍ**회귀분석** : 독립변수와 종속변수 간의 함수적인 관련성을 규명하기 위하여 어떤 수학적 모형을 가정하고, 틍계적으로 추정하는 방법

ㆍy = f(x)의 함수 관계가 있을때, x를 설명변수(explanatory variable)또는 독립변수(independent variable), y를 반응변수(responsive variable)

​	또는 종속변수(dependent variable)이라고 일컬음 / 단순회귀 : 독립변수가 1개, 다중회귀 : 독립변수가 2개 이상

​	다변량 회귀 : 종속변수가 2개이상, 일변량 회귀 : 종속변수가 1개

ㆍ모형 정의 및 가정

- 자료(xi, Yi), i = 1, .... , n에 다음의 관계식이 성립한다고 가정함 
  - Yi = α + βxi + εi(오차항), i = 1, 2, 3, ..... , n
  - Yi ~ N[α + βxi, σ^2] → E[Yi] = α + βxi
- 오차항인 ε1, ε2, ε3, ... , εi는 서로 독립인 확률변수로 εi ~ N[0, σ^2] : 정규, 등분산, 독립 가정
- α, β는 회귀계수라 부르며 α는 절편, β는 기울기를 나타냄 / α, β, σ^2은 미지의 모수로, 상수임

ㆍ모수 추정

- 모형이 포함한 모수 α, β를 추정하기 위하여 각 독립변수 xi에 대응하는 종속변수 yi로 짝지어진 n개의 표본인 관측치(xi, yi)가 주어짐

ㆍ최소제곱법

- 단순회귀모형 Yi = α + βxi + εi에서 자료점과 회귀선 간의 수직거리 제곱합 SS(α, β) = Σ(i=1 ~ n)(yi - α - βxi)^2이 최소가 되도록 추정
- 오차는 실제로 분석하는 데 사용하는 것이 거의 불가능하기때문에 그와 비슷한 값을 가지는 잔차를 활용하여 분석함

ㆍ모형의 유의성 t검정

- 독립변수 x가 종속변수 Y를 설명하기에 유용한 변수인가에 대한 통계적 추론은 회귀계수 β에 대한 검정을 통해 파악할 수 있음.
- 가설 - H0 : β = 0 / H1 : β != 0

ㆍY의 변동성 분해 : SST(yi의 변동) = SSR(모형으로 설명되는 변동) + SSE(모형으로 설명되지 않는 변동)

ㆍ**결정계수** : R^2 = SSR / SST = 1 - (SSE / SST)

- SST = SSR + SSE이므로 항상 0과 1사이의 값을 가짐(0 =< R^2 =< 1)
- yi의 변동 가운데 추정된 회귀모형을 통해 설명되는 변동의 비중을 의미함
- 0에 가까울 수록 추정된 모형의 설명력이 떨어지는 것으로, 1에 가까울 수록 추정된 모형이 yi의 변동을 완벽하게 설명하는 것으로 해석
- R^2는 두 변수 간의 상관계수 r의 제곱과 같음 / 회귀선의 기울기는 β를 이용해서 확인해야함



## 다중회귀분석(Multiple Linear Regression)

ㆍ**다중선형회귀모형**

- 독립변수가 두 개 이상인 선형회귀모형
- 여러개의 독립변수를 이용하면 종속변수의 변화를 더 잘 설명할 수 있을 것임
- 자료((x1i, x2i,  ..... , xki)), i = 1, ...., n에 다음의 관계식이 성립한다고 가정함
  - Yi = α + β1x1i + β2x2i + ..... + βkxki + εi (i = 1, 2, ...., n)
- 정규, 등분산 ,독립을 가정 / 회귀계수 α, β1, .... , βk와 σ^2은 미지인 모수로 상수임
  - βi의 해석 : **xi를 제외한 나머지 모든 예측변수들을 상수로 고정**시킨 후, xi의 한 단위 증가에 따른 E[Y]의 증분을 의미
- 회귀계수 α, β1, ... , βk를 추정
  - 수직거리제곱합 : SS(α, β1, ... , βk) = Σ(i=1 ~ n)(yi - α - β1xi - .... - βkxki)^2이 최소가 되도록 α, β1, ... , βk를 추정
- **R^2 = SSR / SST = 1 - (SSE / SST)**
- 범주형 독립변수가 포함된 회귀모형
  - 범주형 독립변수를 회귀모형에 포함하기 위해서는 더미변수(dummy variable)기법을 사용
  - 더미변수는 0 또는 1의 값을 갖는 변수로 아래와 같이 정의됨 / 더미변수의 개수 = 범주의 개수 - 1



## 다중회귀분석(변수선택)

ㆍ다중회귀모형의 변수선택 : 가능한 적은 수의 설명변수로 좋은 예측력을 가지는 모형을 추정

ㆍ**변수선택법**

- 전진선택법(foward selection)
  - 절편만 있는 모델에서 출발하여 중요한 변수를 하나씩 추가하는 방식
  - 한번 선택된 변수는 제거되지 않는 단점이 존재(새로운 변수가 추가됨으로써 기존모델이 훼손될 수 있는 경우 문제가 됨)
  - 귀무가설을 해당변수가 추가되지않은 모델로, 대립가설을 해당변수가 추가된 모델로 두고 부분 F검정을 시행
  - 유의확률과 p-value를 비교하여 귀무가설 기각여부를 결정하고, 귀무가설이 채택되면 변수 추가를 중지
- 후진제거법(backward elimination)
  - 모든변수가 포함된 모델에서 가장 중요하지 않은 변수부터 하나씩 제거
  - 한번 제거된 변수는 선택되지 않는 단점이 있음
  - 제거여부는 전진선택법과 마찬가지로 부분 F검정을 통해 결정
- **단계선택법(stepwise method)**
  - 절편만 포함된 모델에서 출발해 가장 중요한 변수부터 추가하고, 모델에 포함되어 있는 변수 중에서 중요하지 않은 변수를 제거
  - 새롭게 추가된 변수의 설명력이 더 높고 해당변수에 의해 기존변수의 설명력이 급격하게 줄어들 때 해당 변수를 제거
  - 더 이상 새롭게 추가되는 변수가 없을 때까지 변수의 추가 또는 삭제를 반복함
- 모든 가능한 조합에 대한 회귀모형을 생성한 뒤 가장 적합한 회귀모형을 선택

ㆍ**모형선택의 기준**

- 수정된 결정계수(Adjusted R^2)
  - 결정계수 R^2는 새로운 독립변수가 추가되면 항상 증가함
  - 이를 보완한 수정결정계수 Adjusted R^2는 추가된 독립변수가 종속변수를 설명하는데 기여하는 바가 큰 경우에만 증가
  - Adjusted R^2 = 1 - (SSE / (n - k - 1)) / (SST / (n - 1)) / k는 독립변수의 개수
  - 그 밖에서도 AIC, BIC, Mallow's Cp(작을수록 좋은 지표들임)등의 다양한 적합도 지표를 이용할 수 있음



## 다중회귀분석(잔차분석, 다중공선성)

ㆍ오차에 대한 분포가정 : **정규, 등분산, 독립**(해당 조건을 만족해야 검정이 가능)

ㆍ오차에 대한 가정이 적절한지에 대한 판단이 필요하며, 실제 오차를 구할 수 없으니 잔차를 활용하여 비교

ㆍ**다중공선성** : 독립변수들 간 강한 상관관계가 부정적인 영향을 미치는 현상(적절하게 진단하고 효과를 확인하는 것이 필요)

- 독립변수들간에 강한 선형관계가 존재하는 경우
- 다중회귀모형 분석 시 자주 발생하는 문제 중 하나임
- 다중회귀모형에서 회귀계수 추정에 부정적인 영향을 미침
  - 개별적인 회귀계수 추정의 신뢰성이 떨어져 추정치를믿을 수 없게 만듦
  - 전반적인 모형의 적합성이나 정확도는 크게 변하지 않음
- 진단방법
  - VIF(Variance Inflation Factor) 계수 도출 : VIF = 1 / (1 - Rj^2)
  - Rj^2 : xj 종속변수로 두고 나머지 독립변수로 설명하는 다중선형회귀모델에서의 결정계수
  - VIF계수가 5 또는 10이상인 경우 다중 공선성이 심각한 것으로 봄
- 해결방법
  - 변수선택으로 중복된 변수를 제거
  - 주성분 분석 등을 이용하여 중복된 변수를 변환하여 새로운 변수 생성
  - 릿지, 라쏘 등으로 중복된 변수의 영향력을 일부만 사용

ㆍ다중회귀모형의 가정 위반 검토 및 해결

- 잔차분석 : 회귀모형에서의 가정이 적절한 것인가에 대한 평가
  - 오차의 정규성
  - 오차의 등분산성
  - 오차의 독립성
- 오차는 확률변수로 관찰되지 않는 값이므로, 각 오차에 대응되는 잔차를 관찰한 뒤 잔차들의 분포를 통해 오차에 대하 적정성 평가가능
- 잔차분석방법 : 각 가정 별로, 검정을 통한 방법과 그래프를 통한 시각적인 확인 방법이 가능
  - 시각적 방법을 이용할 경우,
    1. 오차의 정규성 위반 : 히스토그램, QQ플롯
    2. 오차의 등분산성 : 잔차 산점도
    3. 오차의 독립성 : 잔차 산점도(함수적인 패턴이 잔차들의 산점도에 보이면 독립성이 깨진것으로 간주)
  - 가정 위반시 해결방법
    1. 오차의 정규성 위반 : 변수변환
    2. 오차의 등분산성 : 가중최소제곱회귀
    3. 오차의 독립성 : 시계열 분석



## 규제가 있는 선형회귀모델(Ridge, Lasso, Elastic Net)

ㆍ규제가 있는 선형회귀모델 : 파라미터가 너무 커지지않도록 규제하는 추정법(중요하지 않은 변수들을 제거 또는 축소)

ㆍ**선형회귀모델의 규제**

- 모형의 과대적합을 막기위한 규제방법(rehularization)으로 선형회귀모형에서는 보통 모델의 가중치를 제한하는 방법을 사용
  - 선형 회귀모델의 비용함수 : J(β) = (1 / n)Σ(yi - β0 - β1x1i - β2x2i - .... - βkxki)^2
  - 규제가 있는 경우 비용함수 : J(β) = (1 / n)Σ(yi - β0 - β1x1i - β2x2i - .... - βkxki)^2 + λ * penalty(β1, .... , βk)

- 가중치를 제한하는 방법에 따른 규제 선형회귀모델의 종류

  - 릿지회귀(Ridge Regression) - Lp norm 사용 /  p = 1
  - 라쏘회귀(Lasso Regression) - Lp norm 사용 / p = 2
  - 엘라스틱넷(Elastic Net) - 릿지와 라쏘회귀를 섞어서 만든 모델

- **릿지회귀(Ridge Regression)와 L2 규제** / **라쏘회귀(LASSO Regression)와 L1규제**

- 릿지회귀와 라쏘회귀의 특징

  - 두 방식 모두 추정치는 일반선형회귀모형과는 달리 편의가 발생하지만, 분산은 더 작아지게 됨

    → λ에 따라 일반화 오차가 더 작아질 수 있음

  - 라쏘 회귀의 경우 제약 범위가 각진 형태 → 파라미터의 일부가 0이 되는 경향이 있음(sparse model)

  - 릿지 회귀의 경우 제약 범위가 원의 형태 → 파라미터가 0이 되지 않고 전반적으로 줄어드는 경향이 있음.

- **엘라스틱 넷**

  - L1과 L2규제를 혼합한 방식 / 릿지회귀와 라쏘회귀의 장점을 모두 가짐
  - 추정의 관점에서는 더 복잡해지기 때문에 3가지 중 가장 우월하다고 볼 수는 없음



## 분류 : 로지스틱 회귀(Logistic Regression)

ㆍ분류 알고리즘 / 지도학습 알고리즘 / Y가 범주형

ㆍ**로지스틱 회귀모형**

- 로지스틱 회귀분석은 선형 회귀분석과 달리 반응변수가 범주형 데이터인 경우에 사용되는 기법

- 새로운 설명변수의 값이 주어질 때 반응변수의 각 범주에 속할확률을 추정하고 확률을 이용해 분류하는 목적으로 사용됨

- **이항 로지스틱 회귀모형**

  - 이진(0과1)형 값을 가지는 반응변수를 여러 설명변수를 이용하여 회귀식의 형태로 예측하는 모형

  - 반응변수 Y는 1 또는 0의 값을가지는 이진변수, 설명변수는 x1, ...., xk로 k개인 경우에, p = P(Y = 1|x1, .... , xk)라고 하면,

    log(p / (1-p)) = β0 + β1x1 + ..... + βkxk

  - p = (exp(β0 + β1x1+ .... + βkxk)) / (1 + exp(β0 + β1x1+ .... + βkxk)) - 시그모이드 함수

  - 범주형 반응변수의 범주가 두 개일때, 관심범주를 1, 다른 범주를 0으로 정의하면, 반응변수 Y는 관심범주에 속할 확률이

    p인 베르누이 확률분포를 따르는 것을 볼 수 있음 / Pr(Y = 1) = p, Pr(Y = 0) = 1 - p

  - 여기서 확률 p를 독립변수의 함수로 설명하고자 함

  - 확률 p는 0과 1사이의 값이므로, (-INF, INF)의 범위를 가지는 독립변수의 선형함수로 나타낼 수 없음

  - 설명변수가 1개인 경우(k = 1), p = exp(β0 + β1x) / (1 + exp(β0 + β1x))  (β1 > 0)는 비스듬한 S곡선형태를 가짐

    - p는 언제나 0~1사이의 값이 됨
    - x가 -INF 일 때, p = 0 / x가 INF일 때, p = 1이됨

  - 추정 및 예측

    - (xi, yi)개의 표본자료가 주어지면, 최대우도추정법, 경사하강법 등을 이용하여 가장 적합한 곡선 함수(β0, β1)를 추정

  - **로지스틱 회귀모형의 분리경계면**

    - 로지스틱 회귀모형은 선형의 결정 경계를 가짐

    - 독립변수가 2개인 로지스틱 회귀모형과 threshold = 0.5 일때의 초평면(Hyperplane)

      p = e^(β0 + β1x1 + β2x2) / (1 + e^(β0+β1x1+β2x2))

    - 선형으로만 분리가 가능하기때문에 비선형으로 분리가 필요한 경우에는 분리가 불가능

  - 로지스틱 회귀와 오즈비(odds ratio)

    - p = e^(β0 + β1x1 + β2x2) / (1 + e^(β0+β1x1+β2x2)) ↔ p / 1 - p(odds) = e^(β0+β1x1+β2x2)
    - 나머지 변수는 모두 고정시킨 상태에서 한 변수 X1만 1만큼 증가시켰을 때, 변화하는 odds의 비율은 e^β1임을 알 수 있음
    - x1만 1만큼 증가하면, 성공(관심범주, Y = 1)에 대한 오즈가 exp(β1)배 변화함.



## 분류 : 나이브베이즈(Naive Bayes)

ㆍ지도학습, 분류 알고리즘, 특성변수가 주어졌을 때, Y범주의 확률계산에 베이즈 정리를 이용, 생성모델(generative model)

ㆍ**아이디어**

- 목표변수 Y가 2개의 범주 C1, C2를 가진다고 할 때, 특성변수 X의 값을 이용하여 Y의 범주를 예측하는 문제
- X = x로 주어졌을 때 Y의 범주를 예측하는 문제(스팸메일 분류문제에 적용가능)
- P[C1|x] > P[C2|x]면 C1으로 분류하고, 그렇지 않으면 C2로 분류함 / P[Ck|x]는 훈련자료에서 추정하기 어려움(베이즈정리 이용)
- 나이브 베이즈 분류기는 생성(generative)모델임

ㆍ**베이즈 정리의활용**

- P[Ck|x] = P[x ∩ Ck] / P[x] = (P[x|Ck]*P[Ck]) / P[x] / P[x|Ck]와 P[Ck]는 훈련데이터를 이용하여 쉽게 추정할 수 있음

ㆍn개의 특성변수를가지는 분류문제 / 각 특성변수들이 모두 독립이라고 가정하여 P[Ck|x1, .... , xn]의 베이즈 정리를 적용

ㆍ모든 변수가 독립이라는 가정은 비현실적이지만, 스팸메일분류에는 잘 작동함(비현실적이라는 의미에서 나이브[순진한]베이즈라고 함)

ㆍn개의 특성변수를 가지는 분류문제

- P[Ck] : k번째 범주에 속할 확률 / P[xi|Ck] : 목표변수가 k번째 범주일때, 각 특성변수 xi가 관찰될 확률
- xi의 자료형식(범주형/개수형/연속형)에 따라 적절한 확률분포를 가정하여 추정

ㆍ나이브 베이즈의 장단점

- 데이터의 크기가 커도 연산속도가 빠름 / 학습에 필요한 데이터 양이 적어도 좋은 성능을 보이는 편(장점)
- 다양한 텍스트 분류나 추천 등에 활용됨(장점)
- Zero frequency문제나 Underflow문제가 있음 / 모든 독립변수가 독립이라는 가정이 너무 단순함(단점)
- Zero frequency문제는 모든 빈도에 +1을 해주는 라플라스 공식을 적용하면 해결 가능



## 분류 : KNN(K-nearest Neighbor Classifier)

ㆍ**KNN 알고리즘**

- 가장 간단한 지도학습 머신러닝 알고리즘 / 훈련데이터를 저장해 두는 것이 모델을 만드는 과정의 전부임
- K개의 가장 가까운 이웃 중 가장 많은 범주가 어느 곳인가에 따라 범주의 값을 결정

- 새로운 데이터가 입력되면 그 새로운 데이터 주변의 가장 가까운 K개의 훈련 데이터의 레이블을 확인한 뒤 가장 많은 라벨로 분류

- KNN에서 K의 결정은 매우 중요한 문제임

- K가 작으면 이상점 등의 노이즈에 민감하게 반응하는 과적합의 문제 / 너무 크면 자료의 패턴을 잘 파악할 수 없어서 예측 성능이 저하됨

- **검증용(Validation)데이터를 이용하여 주어진 훈련 데이터에 가장 적절한 K를 찾아야 함**

- **거리의 측정**

  - n개의 특성변수를 가지는 자료에서 두개의 관찰점

    - a = (a1, a2, .... , an)와 b = (b1, b2, ..... , bn)간의 거리를 측정하는 문제

    - 유클리디안 거리 = d(a, b) = √(a1 - b1)^2 + (a2 - b2)^2 + ..... + (an - bn)^2

    - 맨해튼 거리 = abs(a1 - b1) + abs(a2 - b2) + ..... + abs(an - bn)

    - 민코우스키 거리 = d(a, b) = (Σ(i = 1 ~ n)abs(ai - bi)^p)^(1/p)

    - 자료의 스케일에 차이가 있는 경우, 스케일이 큰 특성변수에 의해 거리가 결정되어 버릴 수 있음

      따라서 각 특성변수별로 스케일이 유사해지도록 표준화변환(Z-score)또는 min-max변환으로 스케일링을 해줘야함

      Z-score 구하는 법 : (xi - x의 평균 / 표준편차)

